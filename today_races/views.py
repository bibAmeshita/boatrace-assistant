# today_races/views.py

from django.http import JsonResponse, HttpResponseBadRequest
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
from datetime import date
from .models import DailyRaceCache
import logging
logger = logging.getLogger(__name__)

INDEX_URL = "https://www.boatrace.jp/owpc/pc/race/index"
BASE = "https://www.boatrace.jp"

def api_root(request):
    return JsonResponse({
        "status": "ok",
        "endpoints": [
            "/api/today_races/all/",
        ]
    })


WEATHER_URL_DEFAULTS = {
    "æ¡ç”Ÿ": "https://tenki.jp/leisure/horse/3/13/32948/1hour.html",
    "æˆ¸ç”°": "https://tenki.jp/leisure/horse/3/14/32949/1hour.html",
    "æ±Ÿæˆ¸å·": "https://tenki.jp/leisure/horse/3/16/32950/1hour.html",
    "å¹³å’Œå³¶": "https://tenki.jp/leisure/horse/3/16/32951/1hour.html",
    "å¤šæ‘©å·": "https://tenki.jp/leisure/horse/3/16/32952/1hour.html",
    "æµœåæ¹–": "https://tenki.jp/leisure/horse/5/25/32953/1hour.html",
    "è’²éƒ¡": "https://tenki.jp/leisure/horse/5/26/32954/1hour.html",
    "å¸¸æ»‘": "https://tenki.jp/leisure/horse/5/26/32955/1hour.html",
    "æ´¥": "https://tenki.jp/leisure/horse/5/27/32956/1hour.html",
    "ä¸‰å›½": "https://tenki.jp/leisure/horse/4/21/32957/1hour.html",
    "ã³ã‚ã“": "https://tenki.jp/leisure/horse/6/28/32958/1hour.html",
    "ä½ä¹‹æ±Ÿ": "https://tenki.jp/leisure/horse/6/30/32959/1hour.html",
    "å°¼å´": "https://tenki.jp/leisure/horse/6/31/32960/1hour.html",
    "é³´é–€": "https://tenki.jp/leisure/horse/8/39/32961/1hour.html",
    "ä¸¸äº€": "https://tenki.jp/leisure/horse/8/40/32962/1hour.html",
    "å…å³¶": "https://tenki.jp/leisure/horse/7/36/32963/1hour.html",
    "å®®å³¶": "https://tenki.jp/leisure/horse/7/37/32964/1hour.html",
    "å¾³å±±": "https://tenki.jp/leisure/horse/7/38/32965/1hour.html",
    "ä¸‹é–¢": "https://tenki.jp/leisure/horse/7/38/32966/1hour.html",
    "è‹¥æ¾": "https://tenki.jp/leisure/horse/9/43/32967/1hour.html",
    "èŠ¦å±‹": "https://tenki.jp/leisure/horse/9/43/32968/1hour.html",
    "ç¦å²¡": "https://tenki.jp/leisure/horse/9/43/32969/1hour.html",
    "å”æ´¥": "https://tenki.jp/leisure/horse/9/44/32970/1hour.html",
    "å¤§æ‘": "https://tenki.jp/leisure/horse/9/45/32971/1hour.html",
}

# ğŸ ä»Šæ—¥ã®å…¨ãƒ¬ãƒ¼ã‚¹å–å¾—ï¼ˆlocalStorageå´ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
def all_races_today(request):
    if request.method != "GET":
        return HttpResponseBadRequest("GET only")

    from datetime import date
    from .models import DailyRaceCache
    import json

    today = date.today()
    cache = DailyRaceCache.objects.first()

    # âœ… æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã£ã¦ã€ä»Šæ—¥ãªã‚‰ãã®ã¾ã¾è¿”ã™
    if cache and cache.date == today:
        print("ğŸ“¦ ä»Šæ—¥ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ï¼ˆå†å–å¾—ãªã—ï¼‰")
        sites = json.loads(cache.json_text)
        return JsonResponse(sites, safe=False)

    # âš¡ ã“ã“ã‹ã‚‰å–å¾—é–‹å§‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã— or å¤ã„æ—¥ä»˜ï¼‰

    res = requests.get(INDEX_URL, timeout=20)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "html.parser")

    sites = []
    for tbody in soup.select(".table1 table > tbody"):
        try:
            place_img = tbody.select_one("tr td img[alt]")
            place = place_img.get("alt").strip() if place_img else None
            if not place:
                continue

            title_a = tbody.select_one('td.is-alignL.is-fBold.is-p10-7 a[href*="/owpc/pc/race/raceindex"]')
            if not title_a:
                continue

            title = title_a.get_text(strip=True)
            title_url = urljoin(BASE, title_a.get("href"))
            # races = fetch_races_from_raceindex(title_url)

            # ğŸ¯ ãƒ†ã‚¹ãƒˆç”¨ï¼šracesã‚’ç©ºã«ã™ã‚‹ï¼ˆã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼‰
            races = []

            sites.append({
                "place": place,
                "title": title,
                "raceindex_url": title_url,
                "races": races,
            })
        except Exception as e:
            print("Error parsing site:", e)

    # âœ… ç¬¬ä¸€æ®µéšã®JSONä¿å­˜ãƒ†ã‚¹ãƒˆ
    #json_text = json.dumps(sites, ensure_ascii=False, indent=2)
    #with open("test_all_races_today.json", "w", encoding="utf-8") as f:
    #    f.write(json_text)


    # âœ… å®Œå…¨ç‰ˆå–å¾—é–‹å§‹
    for site in sites:
        try:
            site["races"] = fetch_races_from_raceindex(site["raceindex_url"])
            print(f"ğŸ {site['place']}: {len(site['races'])} races å–å¾—")
        except Exception as e:
            print(f"âš ï¸ {site['place']} ã®ãƒ¬ãƒ¼ã‚¹è©³ç´°å–å¾—ã«å¤±æ•—: {e}")

    # âœ… å®Œå…¨ç‰ˆå–å¾—é–‹å§‹
    for site in sites:
        try:
            site["races"] = fetch_races_from_raceindex(site["raceindex_url"])
            print(f"ğŸ {site['place']}: {len(site['races'])} races å–å¾—")
        except Exception as e:
            print(f"âš ï¸ {site['place']} ã®ãƒ¬ãƒ¼ã‚¹è©³ç´°å–å¾—ã«å¤±æ•—: {e}")

    # ğŸŒ¤ å„é–‹å‚¬å ´ã«å¤©æ°—ã‚’ãƒãƒ¼ã‚¸
    for site in sites:
        try:
            merge_weather_into_races(site)
        except Exception as e:
            logger.warning(f"[weather] {site.get('place')} ã¸ã®å¤©æ°—ä»˜ä¸ã«å¤±æ•—: {e}")

    # ğŸ’¾ JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    #json_text = json.dumps(sites, ensure_ascii=False, indent=2)
    #with open("test_all_races_today_full.json", "w", encoding="utf-8") as f:
    #    f.write(json_text)


    # ğŸ’¾ DBã«ä¸Šæ›¸ãï¼ˆå¸¸ã«1ä»¶ï¼‰
    json_text = json.dumps(sites, ensure_ascii=False)
    if cache:
        cache.date = today
        cache.json_text = json_text
        cache.save(update_fields=["date", "json_text", "updated_at"])
        #print(f"ğŸ’¾ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ãä¿å­˜ ({today})")
    else:
        DailyRaceCache.objects.create(date=today, json_text=json_text)
        #print(f"ğŸ†• æ–°è¦ä¿å­˜ ({today})")

    return JsonResponse(sites, safe=False)


# ğŸ å„ä¼šå ´åˆ¥ã®ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
def fetch_races_from_raceindex(url):
    """å„ãƒ¬ãƒ¼ã‚¹å ´ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ï¼ˆ1Rã€œ12Rï¼‰ã‚’å–å¾—"""
    res = requests.get(url, timeout=20)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "html.parser")

    races = []
    rows = soup.select(".contentsFrame1_inner .table1 table tbody tr")
    for row in rows:
        try:
            rno = row.select_one("td.is-fBold a").text.strip()
            time = row.select_one("td:nth-of-type(2)").text.strip()
            racelist_link = row.select_one('ul.textLinks3 a[href*="racelist"]')
            race_url = urljoin(BASE, racelist_link["href"]) if racelist_link else None

            races.append({
                "rno": rno,
                "time": time,
                "url": race_url,
            })
        except Exception as e:
            print("Error parsing race:", e)

    return races


# â˜€ï¸ å„ä¼šå ´ã®å¤©æ°—ã‚’å¤©æ°—äºˆå ±ã‹ã‚‰å–å¾—
def fetch_weather_for_place(place: str):
    """
    tenki.jp ã‹ã‚‰ 1æ™‚é–“ã”ã¨ã®å¤©æ°—ãƒ»é¢¨ã‚’ 1ã€œ24 æ™‚ã® dict ã§è¿”ã™ã€‚
    è¿”ã‚Šå€¤: { hour(int): {"weather": "æ›‡ã‚Š", "direction": "åŒ—è¥¿", "speed": 4}, ... }
    """
    url = WEATHER_URL_DEFAULTS.get(place)
    if not url:
        logger.warning(f"[weather] URL not found for place={place}")
        return {}

    try:
        res = requests.get(url, timeout=15)
        res.encoding = "utf-8"
    except Exception as e:
        logger.warning(f"[weather] request error for {place}: {e}")
        return {}

    soup = BeautifulSoup(res.text, "html.parser")
    table = soup.select_one("#forecast-point-1h-today")
    if not table:
        logger.warning(f"[weather] table not found for {place}")
        return {}

    # æ™‚åˆ»ï¼ˆ01ã€œ24ï¼‰
    hour_cells = table.select("tr.hour td span")
    # å¤©æ°—ï¼ˆæ›‡ã‚Š / æ™´ã‚Œ â€¦ï¼‰
    weather_cells = table.select("tr.weather td p")
    # é¢¨å‘ï¼ˆåŒ—è¥¿ / æ±åŒ—æ± â€¦ï¼‰
    dir_cells = table.select("tr.wind-blow td p")
    # é¢¨é€Ÿï¼ˆ1 / 4 / 7 â€¦ï¼‰
    speed_cells = table.select("tr.wind-speed td span")

    n = min(len(hour_cells), len(weather_cells), len(dir_cells), len(speed_cells))
    result = {}

    for i in range(n):
        try:
            hour = int(hour_cells[i].get_text(strip=True))  # 1ã€œ24
        except ValueError:
            continue

        weather = weather_cells[i].get_text(strip=True)
        direction = dir_cells[i].get_text(strip=True)
        speed_text = speed_cells[i].get_text(strip=True)

        try:
            speed = int(speed_text)
        except ValueError:
            speed = None

        result[hour] = {
            "weather": weather,
            "direction": direction,
            "speed": speed,
        }

    return result

# â˜€ï¸ å¤©æ°—äºˆå ±ã‚’å„ãƒ¬ãƒ¼ã‚¹ã®æ—¥æ™‚ã®ç®‡æ‰€ã«çµåˆ
def merge_weather_into_races(site: dict):
    """
    site = {"place": ..., "races": [...]}
    å„ãƒ¬ãƒ¼ã‚¹ã® time ã‹ã‚‰ hour ã‚’å–ã‚Šå‡ºã—ã¦ã€weather / wind ã‚’è¿½åŠ ã™ã‚‹ã€‚
    """
    place = site.get("place")
    if not place:
        return

    weather_map = fetch_weather_for_place(place)
    if not weather_map:
        return

    for race in site.get("races", []):
        time_str = race.get("time")
        if not time_str:
            continue

        try:
            hour = int(time_str.split(":")[0])  # "08:35" â†’ 8
        except Exception:
            continue

        info = weather_map.get(hour)
        if not info:
            continue

        race["weather"] = info["weather"]

        if info["speed"] is not None:
            race["wind"] = f"{info['direction']}{info['speed']}m"
        else:
            race["wind"] = info["direction"]

#def characters_api(request):
#    from ui.models import Character
#    characters = list(Character.objects.values("id", "name", "tone", "prediction", "index"))
#    return JsonResponse(characters, safe=False)