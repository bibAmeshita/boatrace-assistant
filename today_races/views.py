from django.http import JsonResponse, HttpResponseBadRequest
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
from datetime import date
from .models import DailyRaceCache
import logging
logger = logging.getLogger(__name__)

INDEX_URL = "https://www.boatrace.jp/owpc/pc/race/index"
BASE = "https://www.boatrace.jp"


# ğŸ ä»Šæ—¥ã®å…¨ãƒ¬ãƒ¼ã‚¹å–å¾—ï¼ˆlocalStorageå´ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
def all_races_today(request):
    if request.method != "GET":
        return HttpResponseBadRequest("GET only")

    from datetime import date
    from .models import DailyRaceCache
    import json

    today = date.today()
    cache = DailyRaceCache.objects.first()

    # âœ… æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã£ã¦ã€ä»Šæ—¥ãªã‚‰ãã®ã¾ã¾è¿”ã™
    if cache and cache.date == today:
        print("ğŸ“¦ ä»Šæ—¥ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ï¼ˆå†å–å¾—ãªã—ï¼‰")
        sites = json.loads(cache.json_text)
        return JsonResponse(sites, safe=False)

    # âš¡ ã“ã“ã‹ã‚‰å–å¾—é–‹å§‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã— or å¤ã„æ—¥ä»˜ï¼‰
    logger.info("ğŸ boatrace.jp ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")

    res = requests.get(INDEX_URL, timeout=20)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "html.parser")

    sites = []
    for tbody in soup.select(".table1 table > tbody"):
        try:
            place_img = tbody.select_one("tr td img[alt]")
            place = place_img.get("alt").strip() if place_img else None
            if not place:
                continue

            title_a = tbody.select_one('td.is-alignL.is-fBold.is-p10-7 a[href*="/owpc/pc/race/raceindex"]')
            if not title_a:
                continue

            title = title_a.get_text(strip=True)
            title_url = urljoin(BASE, title_a.get("href"))
            # races = fetch_races_from_raceindex(title_url)

            # ğŸ¯ ãƒ†ã‚¹ãƒˆç”¨ï¼šracesã‚’ç©ºã«ã™ã‚‹ï¼ˆã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼‰
            races = []

            sites.append({
                "place": place,
                "title": title,
                "raceindex_url": title_url,
                "races": races,
            })
        except Exception as e:
            print("Error parsing site:", e)

    # âœ… ç¬¬ä¸€æ®µéšã®JSONä¿å­˜ãƒ†ã‚¹ãƒˆ
    #json_text = json.dumps(sites, ensure_ascii=False, indent=2)
    #with open("test_all_races_today.json", "w", encoding="utf-8") as f:
    #    f.write(json_text)


    # âœ… å®Œå…¨ç‰ˆå–å¾—é–‹å§‹
    for site in sites:
        try:
            site["races"] = fetch_races_from_raceindex(site["raceindex_url"])
            print(f"ğŸ {site['place']}: {len(site['races'])} races å–å¾—")
        except Exception as e:
            print(f"âš ï¸ {site['place']} ã®ãƒ¬ãƒ¼ã‚¹è©³ç´°å–å¾—ã«å¤±æ•—: {e}")

    # ğŸ’¾ JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    #json_text = json.dumps(sites, ensure_ascii=False, indent=2)
    #with open("test_all_races_today_full.json", "w", encoding="utf-8") as f:
    #    f.write(json_text)


    # ğŸ’¾ DBã«ä¸Šæ›¸ãï¼ˆå¸¸ã«1ä»¶ï¼‰
    json_text = json.dumps(sites, ensure_ascii=False)
    if cache:
        cache.date = today
        cache.json_text = json_text
        cache.save(update_fields=["date", "json_text", "updated_at"])
        #print(f"ğŸ’¾ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¸Šæ›¸ãä¿å­˜ ({today})")
    else:
        DailyRaceCache.objects.create(date=today, json_text=json_text)
        #print(f"ğŸ†• æ–°è¦ä¿å­˜ ({today})")

    return JsonResponse(sites, safe=False)


def fetch_races_from_raceindex(url):
    """å„ãƒ¬ãƒ¼ã‚¹å ´ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ï¼ˆ1Rã€œ12Rï¼‰ã‚’å–å¾—"""
    res = requests.get(url, timeout=20)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "html.parser")

    races = []
    rows = soup.select(".contentsFrame1_inner .table1 table tbody tr")
    for row in rows:
        try:
            rno = row.select_one("td.is-fBold a").text.strip()
            time = row.select_one("td:nth-of-type(2)").text.strip()
            racelist_link = row.select_one('ul.textLinks3 a[href*="racelist"]')
            race_url = urljoin(BASE, racelist_link["href"]) if racelist_link else None

            races.append({
                "rno": rno,
                "time": time,
                "url": race_url,
            })
        except Exception as e:
            print("Error parsing race:", e)

    return races


def characters_api(request):
    from ui.models import Character
    characters = list(Character.objects.values("id", "name", "tone", "prediction", "index"))
    return JsonResponse(characters, safe=False)